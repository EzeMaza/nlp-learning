{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Vectorization\n",
    "\n",
    "In this project, I will explore different text vectorization techniques and compare how effective they are in representing textual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Contents\n",
    "\n",
    "1. What is text vectorization?\n",
    "    * Text Vectorization and Text Embedings\n",
    "2. Data colection\n",
    "    * Dataset selection\n",
    "3. Data exploration \n",
    "    * Data visualization\n",
    "4. Preprocessing text data\n",
    "5. TF-IDF Vectorization\n",
    "    * Understanding TF-IDF\n",
    "    * Implementing TF-IDF with scikit-learn\n",
    "    * Code example\n",
    "    * Analyzing most important words per category\n",
    "6. Word Embeddings: Word2Vec\n",
    "    * Introduction to Word2Vec\n",
    "7. Sentence embeddings – BERT and sentence transformers\n",
    "    * Difference between word and sentence embeddings\n",
    "    * BERT\n",
    "        * Using BERT for Sentence Embeddings\n",
    "        * Question Answering (QA)\n",
    "        * Named Entity Recognition (NER)\n",
    "        * Masked Language Modeling (MLM)\n",
    "    * Generate sentence embeddings using SBERT\n",
    "8.  Visualizing vectorized text representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is text vectorization?\n",
    "\n",
    "Text vectorization is the process of *converting text into numerical representations* so that machine learning models and other computational algorithms can process and analyze it. By doing this, operations on sentences become more like math equations, which is something computers can do quickly, and can do well [1].\n",
    "\n",
    "Many tasks that one would like to perform on textual data like text classification, clustering and search engines can be done much more efficiently with numbers rather than words. Since most algorithms operate on numerical data, vectorizing text is crucial for performing these tasks efficiently.\n",
    "\n",
    "#### Text Vectorization and Text Embedings\n",
    "\n",
    "Text vectorization is a much broader term that includes any method that converts text into numerical form. In this regard, **embeddings** are a *specific type* of vectorization. \n",
    "\n",
    "Traditional vectorization methods often rely on *sparse vectors*. For example, if we have a vocabulary consisting of four words: (orange, apple, mango, banana), and we want to represent \"apple\" using one-hot encoding, a possible representation would be [0,1,0,0]. Since the vector size depends on the vocabulary size, if the vocabulary has 100,000 words, each word or document is represented by a 100,000-dimensional vector. However, most of the values in these vectors are zero, leading to inefficiencies in storage and computation (you still have to store and process all those zeroes). Additionally, as dimensionality increases, similarity calculations become less meaningful, making clustering or comparing texts based on meaning more difficult.\n",
    "\n",
    "A major limitation of these methods is that they lack **context awareness** (each word is treated independently, ignoring relationships between them). For example, \"car\" and \"automobile\" would be considered completely different, even though they have similar meanings. Likewise, traditional vectorization methods struggle with **polysemy**, where the same word has multiple meanings depending on context (e.g., \"bank\" as a financial institution vs. \"bank\" as the side of a river). \n",
    "\n",
    "Examples of traditional vectorization methods include **One-Hot Encoding**, **Count Vectorization**, and **TF-IDF**. \n",
    "\n",
    "Text embeddings, on the other hand, represent words or sentences as dense, continuous vectors. Each dimension typically carries meaningful information, enabling the encoding of relationships between words. Lower dimensionality also helps reducing memory usage and speeds up computations.\n",
    "\n",
    "Examples of embeddings are Word2Vec, GloVe, FastText and BERT embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data colection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset selection\n",
    "\n",
    "For this project, I chose to work with the BBC News dataset, which contains more than 2000 pre-categorized news articles across five topics. This dataset is small enough for rapid experimentation while still being large enough to reflect real-world document processing challenges.\n",
    "\n",
    "Unlike other text sources that may contain errors and misspellings, this dataset consists of well-formed sentences. This allows the focus to remain on text representation and modeling techniques rather than data cleaning. News articles offer rich, diverse, and formal text and they include domain-specific terminology, which is great for applying word embeddings and vectorization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_url = \"https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv\"\n",
    "bbc_path = \"bbc-text.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "if not os.path.exists(bbc_path):\n",
    "    response = requests.get(bbc_url)\n",
    "    with open(bbc_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Dataset downloaded successfully.\")\n",
    "else:\n",
    "    print(\"Dataset already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the dataset as a dataframe for easy manipulation\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"bbc-text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe contains the new's category in the column 'category' and the piece of news itself in the 'text' column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "\n",
    "print(df.info())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "df[\"category\"].value_counts().plot(kind=\"bar\", title=\"Category Distribution\", color=\"skyblue\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, the dataset does not contain null values, and the news classes are fairly balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Preprocessing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this crucial step I am going to use the spaCy python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English NLP model\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call nlp on a text, spaCy first tokenizes the text to produce a Doc object. Then the Doc objects is processed in different steps (processing pipeline).\n",
    "\n",
    "The pipeline used by default includes a tagger, a lemmatizer, a parser and an entity recognizer. Each pipeline component returns the processed Doc, which is then passed on to the next component.\n",
    "\n",
    "The disable keyword argument is used to for disabling pipeline components that are not needed [2]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_texts(texts):\n",
    "    processed_texts = []\n",
    "    for doc in nlp.pipe(texts, disable=[\"ner\", \"parser\"]):  \n",
    "        tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_stop]\n",
    "        processed_texts.append(\" \".join(tokens))\n",
    "    return processed_texts\n",
    "\n",
    "df[\"processed_text\"] = preprocess_texts(df[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding TF-IDF\n",
    "\n",
    "TF-IDF (Term Frequency - Inverse Document Frequency) is a statistical measure that evaluates how important a word is to a document in a context of a group of documents. \n",
    "\n",
    "The TF-IDF score is defined as:\n",
    "\n",
    "$$\n",
    "TF-IDF(w) = TF(w) \\times IDF(w)\n",
    "$$\n",
    "\n",
    "Let's begin defining then what each term means.\n",
    "\n",
    "* Term frequency (TF): it measures how often a word (w) appears in a document. Calculating this is pretty straight forward:\n",
    "\n",
    "$$\n",
    "TF(w) = \\frac{\\text{Number of times } w \\text{ appears in a document}}{\\text{Total number of words in the document}}\n",
    "$$\n",
    "\n",
    "\n",
    "* Inverse Document Frequency (IDF): if the importance of a word is measured by it's frequency, then common 'meaningless' words would dominate. To fully capture this, the weight of common words needs to be reduced. This term does exactly that.\n",
    "\n",
    "$$\n",
    "IDF(w) = \\log \\frac{\\text{Total number of documents}}{\\text{Number of documents containing } w}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for instance common words (e.g., \"the\", \"is\") get low scores because they appear in almost all documents, whether \n",
    "important words (e.g., \"Brexit\" in a political article) get higher scores because they appear frequently in fewer documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing TF-IDF with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to use the TfidfVectorizer module from scikit-learn, and apply it to to the the processed_text column. This module does some preprocessing (lowercasing, tokenization), but of course custom preprocessing is necessary  before passing the text in most cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calling fit_transform() the output is a Tf-idf-weighted document-term matrix. After applying TF-IDF vectorization, each document (in this particular case, each news article) is transformed into a numerical vector, where each dimension represents a unique word from the entire dataset. The value in each dimension is the TF-IDF score of that word for that particular document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing the model does is to build the feature space (or 'vocabulary') which entails analyze the entire dataset and extract all unique words. These unique words form the features (or dimensions) of our numerical vector representation.\n",
    "\n",
    "Then, each document is converted into a vector of length equal to the vocabulary size. The value at each position in the vector corresponds to the TF-IDF score of the corresponding word in that document. If a word does not appear in a document, its TF-IDF score is zero for that document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's illustrate this with an example: \n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|}\n",
    "\\hline\n",
    "\\text{category} & \\text{processed\\_tex} \\\\\n",
    "\\hline\n",
    "\\text{tech} & \\text{'tv future hand viewer home theatre system'}  \\\\\n",
    "\\text{business} & \\text{'worldcom boss leave book worldcom boss'}  \\\\\n",
    "\\text{sport} & \\text{'tiger wary farrell gamble leicester rush'} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing, suppose the vocabulary (unique words across all documents) looks like this:\n",
    "\n",
    "$$\n",
    "[\\text{'tv'}, \\text{'future'}, \\text{'hand'}, \\text{'leave'}, \n",
    "\\text{'home'}, \\text{'boss'}, \\text{'rush'}, \\text{'book'},\\text{'wary'}]\n",
    "$$\n",
    "Now, each document is represented as a vector of TF-IDF scores for these words:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|c|c|c|c|c|}\n",
    "\\hline\n",
    "\\text{Document ID} & \\text{tv} & \\text{future} & \\text{hand} & \\text{leave} & \\text{home} & \\text{boss} & \\text{rush} & \\text{book} & \\text{wary} \\\\\n",
    "\\hline\n",
    "1 & 0.50 & 0.60 & 0.60 & 0.00 & 0.50 & 0.00 & 0.00 & 0.00 & 0.00 \\\\\n",
    "2 & 0.00 & 0.00 & 0.00 & 0.50 & 0.00 & 0.60 & 0.00 & 0.50 & 0.00 \\\\\n",
    "3 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.50 & 0.00 & 0.60 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represents a document, and each column corresponds to a word in the vocabulary, with its TF-IDF score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting matrices often contain a lot of zeros. Let's consider the extreme case where the feature space is very large  but each news article is relatively short. In this case, most of the entries in the matrix will be zero, as each document will only contain a small subset of the vocabulary.\n",
    "\n",
    "If we store this matrix as a dense matrix, it would be highly inefficient because we would still need to allocate memory for all those zero values\n",
    "\n",
    "A more memory-efficient solution is to use a sparse matrix representation, such as the Compressed Sparse Row (CSR) format. This format only stores the nonzero values, along with their corresponding row and column indices, which significantly reduces memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF vectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # stick with the 5000 most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the vectorizer and transform the processed text\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df[\"processed_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line follows the format (document_index, word_index) __ TF-IDF score\n",
    "\n",
    "Let's look at the first line:\n",
    "\n",
    "0 → The document index (i.e., first news article).\n",
    "\n",
    "4668 → The column index (i.e., word's position in the vocabulary).\n",
    "\n",
    "0.437 → The TF-IDF score for that word in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to array format for manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_array = tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "\n",
    "tfidf_features = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert TF-IDF matrix to dataframe for convenience\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_array, columns=tfidf_features)\n",
    "\n",
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing most important words per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get the average TF-IDF score for each word across all documents\n",
    "category_means = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "category_means[\"category\"] = df[\"category\"]\n",
    "\n",
    "# Compute mean TF-IDF score for each category\n",
    "category_tfidf = category_means.groupby(\"category\").mean()\n",
    "\n",
    "# Display top words for each category\n",
    "for category in category_tfidf.index:\n",
    "    print(f\"\\nTop words in {category} articles:\")\n",
    "    print(category_tfidf.loc[category].nlargest(10))  # Show top 10 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>observation</ins> : a word that appears in every category is \"say.\" This makes sense, even though it is not a particularly meaningful word in terms of topic-specific content. Given that the dataset consists of news articles, it is common for journalists to cite statements from sources. This frequent attribution of speech may explains why \"say\" is among the most common words across categories. \n",
    "\n",
    "Theoretically \"say\" should have a low TF-IDF score because it's frequency across every category. However, if its TF is extremely high, and the IDF is not low enough, it might still rank highly. One possible approach to adress this is to use a more aggressive IDF weighting or even normalize the score. An even more radical approach would be to add the word as a stop-word for removal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Word embeddings: Word2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've covered TF-IDF, let's move on to word wmbeddings, specifically Word2Vec. As discussed earlier, text embeddings represent words as **low-dimensional**, **dense vectors**, and can capture their relationships in a *continuous vector space*. \n",
    "\n",
    "Unlike traditional text vectorization methods, word embeddings have the advantage that similar words (e.g., king and queen) are positioned closely in the vector space. This enables embeddings to capture semantic relationships between words, which is not possible with simple text vectorization techniques like TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to Word2Vec\n",
    "\n",
    "Word2Vec is a neural network-based approach that learns word relationships by analyzing their context in large collection ot texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by importing Gensim, an open-source Python library  designed for unsupervised topic modeling and NLP tasks, which include, among others, the Word2Vec algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "sentences = df[\"processed_text\"].apply(lambda x: simple_preprocess(x)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "w2v_model.save(\"word2vec_bbc.model\")\n",
    "print(\"Word2Vec model trained and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "w2v_model = Word2Vec.load(\"word2vec_bbc.model\")\n",
    "\n",
    "# Find words most similar to \"government\"\n",
    "print(w2v_model.wv.most_similar(\"government\", topn=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Sentence embeddings – BERT and sentence transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference between word and sentence embeddings\n",
    "\n",
    "Unlike word embeddings, where the transformation is applied to each word individually, sentence embeddings encode entire sentences into a single vector representation.\n",
    "\n",
    "In models like Word2Vec, the relationships between words are often lost. For example, the sentences \"John loves Mary\" and \"Mary loves John\" have very different meanings, yet their word embeddings may not capture this distinction effectively. Word order and sentence semantics are not preserved.\n",
    "\n",
    "In contrast, sentence embeddings retain the full meaning of a sentence. They transform entire sentences into dense, low-dimensional real-valued vectors that capture both word relationships and contextual meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT (Bidirectional Encoder Representations from Transformers) is a contextual language model that processes text while considering both the left and right context of each word.\n",
    "\n",
    "Traditional word embeddings, like those from Word2Vec, generate static word vectors, meaning a word has the same representation regardless of context. In contrast, BERT produces dynamic, context-dependent embeddings: the same word can have different representations depending on the context.\n",
    "\n",
    "BERT is designed for token-level tasks such as named entity recognition, part-of-speech tagging and question answering. It can also be fine-tuned for classification tasks like sentiment analysis and spam detection.\n",
    "\n",
    "##### Using BERT for Sentence Embeddings\n",
    "\n",
    "BERT can generate sentence embeddings using the [CLS] token, but this approach is not optimized for similarity tasks. Comparing two sentences with BERT is computationally expensive because embeddings must be recomputed for every new pair, making it slower than models specifically designed for sentence similarity, such as Sentence-BERT (SBERT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question Answering (QA)\n",
    "\n",
    "In this section I am going to explore the use of the Hugging Face's Transformers library to apply BERT for question qnswering on the BBC dataset. Transformers provides APIs and tools to download and train state-of-the-art pretrained models for NLP tasks, computer vision among others [3].\n",
    "\n",
    "The model will take a context (a news article) and a question as inputs and will find the exact answer in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an article as context\n",
    "\n",
    "context = df.loc[4, \"text\"]  \n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the question answering pipeline\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pipeline** is a high-level API that simplifies the use of pre-trained deep learning models for NLP tasks.\n",
    "\n",
    "The argument **\"question-answering\"** specifies that we want to use a model for extracting answers from a given context.\n",
    "\n",
    "We use distilbert-base-cased-distilled-squad, a lightweight but powerful BERT model for QA.\n",
    "**\"distilbert-base-cased-distilled-squad\"** is a DistilBERT model fine-tuned on the SQuAD (Stanford Question Answering Dataset). It is a smaller and faster version of BERT that retains almost the same performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's think about some appropiate questions\n",
    "\n",
    "questions = [\n",
    "    \"Who stars in Ocean's Twelve?\",\n",
    "    \"How much did Ocean’s Twelve earn in its opening weekend at the US box office?\",\n",
    "    \"Which film did Ocean’s Twelve surpass to become number one at the US box office?\",\n",
    "    \"Who directed Ocean’s Twelve?\",\n",
    "    \"How did US critics react to Ocean’s Twelve?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get answers from BERT\n",
    "\n",
    "for question in questions:\n",
    "    answer = qa_pipeline(question=question, context=context)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer['answer']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT got the director right and provided a somewhat reasonable answer for the critics' reaction, but it made significant errors in identifying the cast, box office earnings, and the film Ocean’s Twelve surpassed.\n",
    "\n",
    "The model predicts answers based on context. Because it processes text in chunks, this sometimes leads to misinterpretation when multiple similar entities (e.g., multiple numbers, multiple names) are together. It misidentified \"Steven Soderbergh\" as an actor, likely because his name appeared near the cast list. It also incorrectly pulled \"$110m\" instead of \"$40.8m\" maybe because \"$110m\" appears later in the text. \n",
    "\n",
    "BERT is decent for simple fact extraction, but it’s not great at reasoning, handling numbers, or distinguishing subtle relationships in a complex text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Named Entity Recognition (NER) with BERT\n",
    "\n",
    "NER identifies people, organizations, locations, and more in the BBC articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Select a different article to analyze\n",
    "\n",
    "ner_text = df.loc[2, \"text\"]\n",
    "print(ner_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize  NER pipeline \n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also tried the model dbmdz/bert-large-cased-finetuned-conll03-english. The thing is that this particular model is *cased*, meaning it expects properly capitalized words (e.g., \"Andy Farrell\" instead of \"andy farrell\"). If the input is all lowercase, it may fail to recognize named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NER\n",
    "\n",
    "entities = ner_pipeline(ner_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for entity in entities:\n",
    "    print(f\"Entity: {entity['word']}, Type: {entity['entity']}, Score: {entity['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first detected 'word' is 'far', and is being recognized as B-PER (beginning of a person’s name) with moderate confidence (score: 0.83). 'far' is likely a truncated part of 'Farrell', which means the model is not properly recognizing full names.\n",
    "\n",
    "The second detected word is another instance of \"far\" being misclassified as a person's name.\n",
    "\n",
    "The third 'word' is 'en', recognized as B-ORG (beginning of an organization’s name) but this time with very low confidence\n",
    "(Score: 0.46). The most likely issue here is that 'en'is a fragment of another word (maybe \"England\" or \"Leicester\") that got cut. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is recognizing fragments (\"far\" from \"Farrell\" and \"en\" from something else) instead of full entities. This happens because transformers tokenize words into *subwords*, and if the model isn't trained well on reassembling them, it gives partial results. Another issue thay may affect the model's performance is that the input is *lowercased*. If possible, the input should always be properly cased for NER tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, not everything is lost. From the output, it is possible to extract the token’s position in the sequence. If a token is suspected to be part of a relevant word, the complete word can be reconstructed by combining adjacent tokens.\n",
    "\n",
    "Another possible approach is to split the text into sentences before processing, rather than analyzing the entire text at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked Language Modeling (MLM) \n",
    "\n",
    "MLM allows us to predict missing words in a sentence and it has practical applications in auto-completion and text suggestion for search engines, virtual assistants, and code editors. MLM is useful for spell checking and grammar correction as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLM pipeline\n",
    "\n",
    "mlm_pipeline = pipeline(\"fill-mask\", model=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sentence from a BBC article and mask a word\n",
    "\n",
    "mlm_text = \"the way [MASK] watch tv will be radically different in five years  time\"\n",
    "print(mlm_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "\n",
    "predictions = mlm_pipeline(mlm_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top predictions\n",
    "for pred in predictions:\n",
    "    print(f\"Predicted Word: {pred['token_str']}, Confidence: {pred['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_text_expanded = \"tv future in the hands of viewers with home theatre systems  plasma high-definition tvs  and digital video recorders moving into the living room  the way [MASK] watch tv will be radically different in five years  time.  that is according to an expert panel which gathered at the annual consumer electronics show in las vegas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for the expanded phrase\n",
    "\n",
    "predictions_expanded = mlm_pipeline(mlm_text_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top predictions\n",
    "for pred in predictions_expanded:\n",
    "    print(f\"Predicted Word: {pred['token_str']}, Confidence: {pred['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the original sentence, the model is uncertain about the subject and predicts \"you\" (37%), \"we\" (26%), \"I\" (24%), etc. This uncertainty is reflected in the *relatively even distribution* of probabilities.\n",
    "\n",
    "In contrast, when using the expanded sentence, \"they\" (73%) becomes the dominant prediction with much higher confidence. This suggests that the model has recognized references to \"viewers\" in the preceding text, making \"they\" the choice in this context. The probabilities for \"you\" and \"we\" decrease, highlighting the fact that the model can now discenr that the sentence is referring to a third-person group.\n",
    "\n",
    "This shift in predictions demonstrates how added context help resolve this ambiguity. This is a good example of how BERT makes use of contextual information to refine its word predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate sentence embeddings using SBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SBERT (Sentence-BERT) is a variant of the BERT model designed specifically for generating **sentence embeddings**.\n",
    "\n",
    "Unlike traditional BERT, which as we saw earlier is optimized for token-level tasks, SBERT fine-tunes BERT to produce meaningful sentence-level representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained SBERT model\n",
    "\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all-MiniLM-L6-v2 is a lightweight SBERT model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each article, generate embeddings\n",
    "\n",
    "df['sbert_embedding'] = df['processed_text'].apply(lambda x: sbert_model.encode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>References<ins>:\n",
    "\n",
    "[1] https://www.ibm.com/docs/en/watsonx/saas?topic=embeddings-text-overview\n",
    "\n",
    "[2] https://spacy.io/usage/processing-pipelines\n",
    "\n",
    "[3] https://huggingface.co/docs/transformers/en/index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
