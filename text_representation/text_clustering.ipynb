{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Clustering\n",
    "\n",
    "Text clustering is a major task in NLP and in other areas, and is one of the most common task for unspervised learning\n",
    "\n",
    "In this notebook, I am going to apply **unsupervised learning** techniques to group similar news articles from the BBC News dataset. Using different **text vectorization methods**, I am going to explore how well various clustering algorithms can categorize the news articles into meaningful groups.\n",
    "\n",
    " What Youâ€™ll Learn by Doing This Project\n",
    "How to apply clustering algorithms to text data.\n",
    "The impact of different text representations on clustering quality.\n",
    "How to evaluate clustering performance (even without predefined labels).\n",
    "The practical challenges of unsupervised learning in NLP.\n",
    "\n",
    "## Steps Overview\n",
    "1. **Load & Prepare Data**  \n",
    "   - Load preprocessed text and vectorized representations (TF-IDF, Word2Vec, SBERT).\n",
    "\n",
    "2. **Apply Clustering Algorithms**  \n",
    "   - Use **K-Means**, **Hierarchical Clustering**, and **DBSCAN** to identify clusters.\n",
    "\n",
    "3. **Evaluate Clustering Performance**  \n",
    "   - Compute **Silhouette Score**, **Davies-Bouldin Index**, and **Adjusted Rand Index (ARI)** (if labels are available).\n",
    "\n",
    "4. **Visualize Clusters**  \n",
    "   - Reduce dimensionality using **PCA or t-SNE**.\n",
    "   - Plot cluster distributions to analyze how well-separated they are.\n",
    "\n",
    "5. **Compare Embedding Methods**  \n",
    "   - Evaluate clustering results for **TF-IDF, Word2Vec, and SBERT**.\n",
    "   - Discuss which embedding method is most effective for clustering news articles.\n",
    "\n",
    "## ðŸ›  Tech Stack\n",
    "- **Python** (pandas, numpy, matplotlib, seaborn)\n",
    "- **scikit-learn** (K-Means, evaluation metrics)\n",
    "- **gensim** (Word2Vec)\n",
    "- **sentence-transformers** (SBERT embeddings)\n",
    "- **hdbscan** (Density-based clustering)\n",
    "\n",
    "## ðŸŽ¯ Expected Outcome\n",
    "- A clear comparison of **how different embeddings affect clustering**.\n",
    "- Identification of **the best approach for grouping news articles** without predefined labels.\n",
    "- Insights into **how well clusters align with actual news categories**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Load & Prepare Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"bbc-text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")  # load the english NLP model\n",
    "\n",
    "def preprocess_texts(texts):\n",
    "    processed_texts = []\n",
    "    for doc in nlp.pipe(texts, disable=[\"ner\", \"parser\"]):  \n",
    "        tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_stop]\n",
    "        processed_texts.append(\" \".join(tokens))\n",
    "    return processed_texts\n",
    "\n",
    "df[\"processed_text\"] = preprocess_texts(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF \n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # stick with the 5000 most common words\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df[\"processed_text\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec \n",
    "\n",
    "texts = df[\"processed_text\"].tolist()\n",
    "w2v_model = Word2Vec(sentences=[text.split() for text in texts], vector_size=300, window=5,  min_count=2, workers=4)\n",
    "word_vectors = np.array([np.mean([w2v_model.wv[word] for word in text.split() if word in w2v_model.wv], axis=0) for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT\n",
    "\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "df['sbert_embedding'] = df['processed_text'].apply(lambda x: sbert_model.encode(x))\n",
    "sbert_embeddings = np.stack(df['sbert_embedding'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Apply Clustering Algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means clustering\n",
    "\n",
    "K-means is an unsupervised clustering algorithm that partitions data into k clusters by iteratively updating cluster centroids to minimize intra-cluster variance (sum of squared distances). It is simple, easy to implement, and efficient even on large datasets, making it widely used. On the flip side, K-means performs best when clusters are roughly spherical and of similar size, which is not always the case. Additionally, the number of clusters (k) must be specified beforehand, which can be a challenging problem all by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to select the number of clusters to initialize K-Means algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying the K-Means algorithm, I will use both the **elbow method** and the **silhouette score** to determine the optimal number of clusters. Since they have different strengths, using them together provides a more well-rounded analysis, especially in cases where the optimal k is not clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elbow method\n",
    "\n",
    "It measures how compact clusters are by evaluating within-cluster variance (or inertia). This technique helps determine the optimal number of clusters by identifying the point where adding more clusters no longer significantly reduces inertia (this is a key point to make since adding more clusters always reduces the distance between points and their assigned centroids). In this regard detecting the elbow isn't always clear. \n",
    "\n",
    "This method is particularly useful for datasets with well-defined clusters, as it focuses solely on how tightly data points are grouped rather than how well-separated the clusters are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elbow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes into account both cohesion (similarity inside the cluster) and separation (differences between clusters). A higher score means better-defined clusters. For a suscint and clear explanation see [here](https://www.youtube.com/watch?v=a2Kg2_l3L8M) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the elbow method to the vectors generated with the TF-IDF method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow.elbow_method(tfidf_matrix, max_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to interpret this plot?**\n",
    "\n",
    "This plot shows the inertia as a function of the number of clusters. The fact that the inertia is a monotonic decreasing function is expected.\n",
    "\n",
    "Here we have to look for the \"elbow\" point: this is where the rate of decrease in inertia slows down significantly. In this plot however, there is no clear elbow. The decrease appears quite linear (with the same slope value that is) across all the domain of k. But the largest drop happens between $k = 1$ and $k = 3$ and after that the reduction appears more gradual. If forced to choose based on this plot alone, $k=3$ might be reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the elbow method appears to be non-conclusive, let's check now the **silhouette score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette.silhouette_method(tfidf_matrix, max_k=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to interpret this plot?**\n",
    "\n",
    "As mentioned before, the silhouette score measures clustering quality ( higher values indicate better defined and well separated clusters)\n",
    "\n",
    "The silhouette score at $k = 3$ is low compared to higher k values. This suggest that increasing k improves clustering quality.\n",
    "\n",
    "At $k = 8$ the silhouette score reaches a local maximum, which might indicate distinct and well-separated clusters.\n",
    "\n",
    "The drop after this value may suggests this well defined clusters are being split into smaller, less meaningful ones (this is known as *over-segmentation*)\n",
    "\n",
    "The increase around $k = 14$ could mean that there is an *underlying substructure*, and subsequent partition of the data helps to capture this. However, too high k risks overfitting by artificially creating more clusters than needed.\n",
    "\n",
    "\n",
    "**Why is the silhouette score more reliable in this case?**\n",
    "\n",
    "The elbow method only considers inertia, which measures how compact clusters are. However, compact clusters do not always mean good separation. The silhouette score takes into acount both intra and inter cluster distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply this methods to the vectors generated with Word2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply elbow method\n",
    "\n",
    "elbow.elbow_method(word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $k = 4$ or $k = 5$ are good candidates: inertia decreases much more slowly for $k > 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate silhouette score \n",
    "\n",
    "silhouette.silhouette_method(word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette score is the greatest for $k = 4$. In this case, the k's obtained with both methods coincide!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, apply this methods to the vectors generated with SBERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply elbow method\n",
    "\n",
    "elbow.elbow_method(sbert_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $k = 4$, $k = 5$ or even $k = 7$ are possible candidates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate silhouette score \n",
    "\n",
    "silhouette.silhouette_method(sbert_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying a similar criteria as the one in the first case, I will select $k = 4$ as the best candidate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the number of clusters for initializing K-Means for the three arrays of vectors are:\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|}\n",
    "\\hline\n",
    "\\text{\\# clusters} & \\text{TF-IDF} & \\text{Word2Vec} & \\text{SBERT} \\\\\n",
    "\\hline\n",
    "k & 8 & 4 & 4 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to apply the K-Means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means clustering to the generated vectors\n",
    "\n",
    "tfidf_labels, tfidf_inertia = kmeans.kmeans_clustering(tfidf_matrix, k=8)\n",
    "word_labels, word_inertia = kmeans.kmeans_clustering(word_vectors, k=4)\n",
    "sbert_labels, sbert_inertia = kmeans.kmeans_clustering(sbert_embeddings, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering builds a *hierarchy of clusters* that can be cut at different levels to obtain different groupings. It works in two main ways, top-down and bottom-up (agglomerative) approaches, with the bottom-up method being the most common.\n",
    "\n",
    "The result of this process is a tree-like graph called a **dendrogram**.  This kind of plots give a clear visualization of data relationships, for instance, the height at which two clusters merge represents their distance. By cutting the dendrogram at a specific height, you can determine the final clusters.\n",
    "\n",
    "One strength of hierarchical clustering, compared to methods like k-means, is that it doesnâ€™t require predefining the number of clusters. On the flip side it becomes computationally expensive for large datasets and is sensitive to noise and outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
