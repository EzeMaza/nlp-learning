{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning & Preprocessing Pipeline\n",
    "\n",
    "In this project, I will build a Python script to process raw text files by applying basic NLP techniques. The goal is to transform unstructured text into a cleaner, more structured format, making it more suitable for further analysis.\n",
    "\n",
    "The script will take a raw text file and apply fundamental NLP preprocessing steps to extract meaningful information such as keywords and named entities. These techniques will enhance the text’s usability for advanced NLP tasks like summarization, sentiment analysis, or topic modeling.\n",
    "\n",
    "This project aims to provide hands-on experience with key NLP concepts, tools, and libraries. The main steps include:\n",
    "\n",
    "- **Tokenization**: Splitting text into words or sentences.\n",
    "- **Stopword Removal**: Removing common words (e.g., \"the,\" \"is,\" \"and\") that add little meaning.\n",
    "- **Stemming & Lemmatization**: Reducing words to their root forms.\n",
    "- **Part-of-Speech (POS) Tagging**: Assigning grammatical categories to words.\n",
    "- **Named Entity Recognition (NER)**: Identifying proper names, places, and organizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Contents\n",
    "\n",
    "1. Read & Load the Text Data\n",
    "\n",
    "2. Tokenization\n",
    "\n",
    "3. Stopword Removal\n",
    "\n",
    "4. Stemming & Lemmatization\n",
    "\n",
    "5. Part-of-Speech (POS) Tagging\n",
    "\n",
    "6. Keyword Extraction\n",
    "    * Named Entity Recognition (NER)\n",
    "    * Noun Chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read & Load the Text Data\n",
    "\n",
    "First I will load a text file (in .txt format) and remove whitespaces, special characters and extra line breaks. Also I will convert the text to lowercase for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why cleaning text is a crucial preprocessing step in Natural Language Processing?\n",
    "\n",
    "Algorithms do not interpret text the same way as people do. For me and you, separation between words is necessary to understarn what is written. For NLP models they only add noise and do not have meaning whatsoever. \n",
    "\n",
    "Text preprocessing ensures that only the parts that carry meaning are feed to the model, making it perform better. \n",
    "\n",
    "Of course this has to be carry out with some care: for instance, when lowercasing. Apple and apple are the same word but, in some context, they can have different meaning (the first may be referring to the company and the later to the fruit) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text file\n",
    "\n",
    "def load_text(file_path):\n",
    "    \"\"\"Reads a text file and returns the content as a string.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/ezequ/Documents/Python/projects/intelligence.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = load_text(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans raw text by removing special characters, extra spaces, and converting to lowercase.\"\"\"\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)  # Remove references \n",
    "    text = re.sub(r'[,:;]', '', text)  # Remove commas\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces and newlines\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = clean_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Text:\\n\", raw_text)\n",
    "print(\"\\nCleaned Text:\\n\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a cleaned version of the original text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What Tokenization means?\n",
    "\n",
    "Is the process of *splitting text* into smaller units. Sentence tokenization means splitting a text into sentences, wheras word tokeniazation means splitting sentences into individual words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK and spaCy\n",
    "\n",
    "Both NLTK and spaCy are Python libraries for Natural Language Processing (NLP), but they have different purposes.\n",
    "\n",
    "NLTK is flexible and includes a wide range of NLP algorithms, making it ideal for research and educational use. However, its extensive resources often require more manual tuning, and it is not optimized for large-scale, industrial applications.\n",
    "\n",
    "spaCy, on the other hand, is designed for speed and production-ready applications. While it is less flexible than NLTK for custom text processing, it is significantly faster and comes with efficient pre-trained language models (which need to be downloaded separately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for tokenization\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required resources\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tokenize(text):\n",
    "    \"\"\"Tokenizes text into sentences and words using NLTK.\"\"\"\n",
    "    sentences = sent_tokenize(text)  # Sentence tokenization\n",
    "    words = word_tokenize(text)  # Word tokenization\n",
    "    return sentences, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the cleaned version of the text\n",
    "\n",
    "sentences, words = nltk_tokenize(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sentence Tokenization (First 3 sentences):\")\n",
    "for i, sentence in enumerate(sentences[:3], 1):\n",
    "    print(f\"{i}. {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Word Tokenization (First 20 words):\")\n",
    "print(words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using SpaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenize(text):\n",
    "    \"\"\"Tokenizes text into sentences and words using spaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]  # Sentence tokenization\n",
    "    words = [token.text for token in doc]  # Word tokenization\n",
    "    return sentences, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_sentences, spacy_words = spacy_tokenize(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"spaCy Sentence Tokenization (First 3 sentences):\")\n",
    "for i, sentence in enumerate(spacy_sentences[:3], 1):\n",
    "    print(f\"{i}. {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nspaCy Word Tokenization (First 20 words):\")\n",
    "print(spacy_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_match = True  \n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    if sentence not in spacy_sentences:\n",
    "        print(f\"Sentence number {i} is not equal to the sentence using the spaCy model\")\n",
    "        all_match = False  \n",
    "\n",
    "if all_match:\n",
    "    print(\"Sentence tokenization is the same with both models\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_words = []\n",
    "\n",
    "for word in words:\n",
    "    if word not in spacy_words:  \n",
    "        different_words.append(word)\n",
    "\n",
    "print(different_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, NLTK manage better compund words like 'self-awareness' and 'problem-solving'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stopword Removal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are commonly occurring words like \"the\", \"is\", and \"and\" that don’t carry much meaningful information in many NLP tasks. Removing them helps reduce noise and improve the performance of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK stopwords\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_remove_stopwords(words):\n",
    "    \"\"\"Removes stopwords from the tokenized words using NLTK.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = nltk_remove_stopwords(words)\n",
    "\n",
    "print(\"Words after Stopword Removal (First 20 words):\")\n",
    "print(filtered_words[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now with spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_remove_stopwords(words):\n",
    "    \"\"\"Removes stopwords from the tokenized words using spaCy.\"\"\"\n",
    "    stop_words = nlp.Defaults.stop_words\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words_spacy = spacy_remove_stopwords(spacy_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Words after Stopword Removal (First 20 words):\")\n",
    "print(filtered_words_spacy[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and lemmatization are text preprocessing techniques that reduce word variants to one base form [1]. Even though the goal is the same, the approach is different: **stemming** merely removes common suffixes from the end of word tokens, whereas **lemmatization** ensures the output word is an existing, normalized form that can be found in the dictionary.\n",
    "\n",
    "There is a trade-off between these two processes. Lemmatization may seem like a more robust technique—because it is. However, the drawback is that it is more computationally intensive. **Stemming** is preferable when **speed matters** and some errors are acceptable (e.g., in large-scale text processing, quick search indexing). **Lemmatization** is better for **accuracy**, especially when dealing with tasks requiring proper word meaning (e.g., NLP models, sentiment analysis, and linguistics-heavy applications). If computational cost is not an issue, **lemmatization is usually the better choice** since it preserves the actual dictionary form of words.\n",
    "\n",
    "Why is this necessary?\n",
    "Natural language is highly redundant, and *a single concept can be represented in multiple ways* (e.g., run, running, ran, runs). Some words have multiple inflected forms (study vs. studies, am vs. is vs. are) that may end up being treated as separate words. **Reducing the number of unique words in a dataset while retaining their true meaning** ensures that similar words are recognized as the same. This has the added benefit of requiring **less memory and computation**, making text processing more efficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will apply stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_stemming(words):\n",
    "    \"\"\"Applies stemming to words using NLTK's SnowballStemmer.\"\"\"\n",
    "    stemmed_words = [SnowballStemmer(\"english\").stem(word) for word in words]\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words = nltk_stemming(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Words after Stemming (First 20 words):\")\n",
    "print(stemmed_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many, many 'weird' words in this list!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try lemmanizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download WordNet for lemmatization\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_lemmatization(words):\n",
    "    \"\"\"Applies lemmatization using NLTK's WordNetLemmatizer.\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words_nltk = nltk_lemmatization(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Words after Lemmatization:\")\n",
    "print(lemmatized_words_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lemmatization(words):\n",
    "    \"\"\"Applies lemmatization using spaCy.\"\"\"\n",
    "    doc = nlp(\" \".join(words)) \n",
    "    lemmatized_words = [token.lemma_ for token in doc]\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words_spacy = spacy_lemmatization(filtered_words_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Words after Lemmatization with spaCy (First 20 words):\")\n",
    "print(lemmatized_words_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Part-of-Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tagging is the process of labeling words with their grammatical roles, such as nouns, verbs, adjectives and adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_pos_tagging(words):\n",
    "    \"\"\"Assigns Part-of-Speech (POS) tags using NLTK.\"\"\"\n",
    "    return pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags_nltk = nltk_pos_tagging(lemmatized_words_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"POS Tags (First 20 words):\")\n",
    "print(pos_tags_nltk[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_pos_tagging(words):\n",
    "    \"\"\"Assigns Part-of-Speech (POS) tags using spaCy.\"\"\"\n",
    "    doc = nlp(\" \".join(words))  \n",
    "    return [(token.text, token.pos_) for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags_spacy = spacy_pos_tagging(lemmatized_words_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"POS Tags (First 10 words using spaCy):\")\n",
    "print(pos_tags_spacy[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to compare how the 2 models classify words, in particular for nouns verbs and adjectives, which by common experience are the most common types of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouns\n",
    "\n",
    "noun_list_ntk = {word[0] for word in pos_tags_nltk if word[1] == 'NN'}\n",
    "noun_list_spacy = {word[0] for word in pos_tags_spacy if word[1] in ('NOUN', 'PROPN')}\n",
    "\n",
    "diff_nltk = [word for word in noun_list_ntk if word not in noun_list_spacy]\n",
    "diff_spacy = [word for word in noun_list_spacy if word not in noun_list_ntk]\n",
    "\n",
    "print(\"These words are recognized as nouns in the NLTK model but not in the spaCy model:\", diff_nltk)\n",
    "print(\"These words are recognized as nouns in the spaCy model but not in the NLTK model:\", diff_spacy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbs\n",
    "\n",
    "verb_list_ntk = {word[0] for word in pos_tags_nltk if word[1] == 'VBD'}\n",
    "verb_list_spacy = {word[0] for word in pos_tags_spacy if word[1] in ('VERB')}\n",
    "\n",
    "verb_diff_nltk = [word for word in verb_list_ntk if word not in verb_list_spacy]\n",
    "verb_diff_spacy = [word for word in verb_list_spacy if word not in verb_list_ntk]\n",
    "\n",
    "print(\"These words are recognized as verbs in the NLTK model but not in the spaCy model:\", verb_diff_nltk)\n",
    "print(\"These words are recognized as verbs in the spaCy model but not in the NLTK model:\", verb_diff_spacy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjectives\n",
    "\n",
    "adj_list_ntk = {word[0] for word in pos_tags_nltk if word[1] == 'JJ'}\n",
    "adj_list_spacy = {word[0] for word in pos_tags_spacy if word[1] == 'ADJ'}\n",
    "\n",
    "adj_diff_nltk = [word for word in adj_list_ntk if word not in adj_list_spacy]\n",
    "adj_diff_spacy = [word for word in adj_list_spacy if word not in adj_list_ntk]\n",
    "\n",
    "print(\"These words are recognized as adjectives in the NLTK model but not in the spaCy model:\", adj_diff_nltk)\n",
    "print(\"These words are recognized as adjectives in the spaCy model but not in the NLTK model:\", adj_diff_spacy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, I can conclude that:\n",
    "\n",
    "* NLTK did a good job with **compound words**, correctly recognizing and classifying them.\n",
    "* spaCy dig great at lemmatization, assigning present-tense verb forms, which later proved crucial for classification.\n",
    "* Overall, spaCy performed better, with fewer misclassifications and more effective lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this reasons, I will stick to spaCy from here on now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Keyword Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition (NER)\n",
    "\n",
    "This is the part in which we identify and categorize *named entities*, such as names of people, organizations, locations and dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_ner(text):\n",
    "    \"\"\"Extracts named entities from text using spaCy.\"\"\"\n",
    "    doc = nlp(text)  # Convert text to a spaCy document\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove text references\n",
    "\n",
    "text_nref = re.sub(r'\\[\\d+\\]', '', raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_spacy = spacy_ner(text_nref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Named Entities:\")\n",
    "print(entities_spacy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noun Chunks\n",
    "\n",
    "These are phrases that usually contain key concepts (e.g., \"machine learning model\", \"financial market trends\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_spacy(text, top_n=10, min_length=4):\n",
    "    \"\"\"Extracts keyword phrases using spaCy's noun chunks.\"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    keywords = [chunk.text.lower() for chunk in doc.noun_chunks if len(chunk.text) >= min_length]\n",
    "    keywords = list(set([kw for kw in keywords if kw.isalpha() and kw not in nlp.Defaults.stop_words]))\n",
    "\n",
    "    keyword_counts = Counter(keywords).most_common(top_n)\n",
    "    \n",
    "    return keyword_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Keywords (Noun Chunks):\")\n",
    "print(extract_keywords_spacy(text_nref))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>References<ins>:\n",
    "\n",
    "[1] https://www.ibm.com/think/topics/stemming-lemmatization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
